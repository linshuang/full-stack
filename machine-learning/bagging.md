# Bagging (bootstrap aggregating)

Bagging即套袋法，Bagging要求“不稳定”（不稳定是指数据集的小的变动能够使得分类结果的显著的变动）的分类方法。
其算法过程如下：

A）从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k个训练集.（k个训练集相互独立）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回归方法，如决策树、神经网络等）

C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果. 

# bagging和boosting的差异
1）样本选择上：
```
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。<br/>
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
```

2）样例权重：
```
Bagging：使用均匀取样，每个样例的权重相等<br/>
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
```

3）预测函数：
```
Bagging：所有预测函数的权重相等。<br/>
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
```

4）并行计算：
```
Bagging：各个预测函数可以并行生成<br/>
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
```

# 为什么说bagging是减少variance，而boosting是减少bias?
bagging用多个训练于不同训练集的独立模型的整合来对抗模型在训练集扰动情况下的不稳定性，最终结果就是模型更稳定，其实这种想法跟使用超大数据集来获得更小variance的理念有点类似；而boosting由于后一模型依赖于前一模型（前向），最终的模型实际上是增加了复杂度的，增加模型复杂度可能导致增加variance，但能够减少bias。
https://www.zhihu.com/question/26760839